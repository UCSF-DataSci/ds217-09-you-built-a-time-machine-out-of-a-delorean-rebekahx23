{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcd9864b",
   "metadata": {},
   "source": [
    "# Question 2: Resampling and Frequency Conversion\n",
    "\n",
    "This question focuses on resampling operations and frequency conversion using ICU monitoring data (hourly) and patient vital signs data (daily).\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d645281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a454b02",
   "metadata": {},
   "source": [
    "## Part 2.1: Load and Prepare Data\n",
    "\n",
    "**Note:** These datasets have realistic characteristics:\n",
    "- **ICU Monitoring**: 75 patients with variable stay lengths (2-30 days). Not all patients are present for the entire 6-month period - patients are admitted and discharged at different times.\n",
    "- **Patient Vitals**: Already contains some missing visits (~5% missing data). This is realistic and will be useful for practicing missing data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b166d302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICU monitoring shape: (86400, 7)\n",
      "Patient vitals shape: (18250, 7)\n",
      "\n",
      "ICU monitoring sample:\n",
      "                    patient_id  heart_rate  blood_pressure_systolic  \\\n",
      "datetime                                                              \n",
      "2023-01-01 00:00:00     ICU001   82.000000                      126   \n",
      "2023-01-01 01:00:00     ICU001   98.294095                      128   \n",
      "2023-01-01 02:00:00     ICU001  103.500000                      129   \n",
      "2023-01-01 03:00:00     ICU001   91.535534                      136   \n",
      "2023-01-01 04:00:00     ICU001   87.330127                      129   \n",
      "\n",
      "                     blood_pressure_diastolic  oxygen_saturation  temperature  \n",
      "datetime                                                                       \n",
      "2023-01-01 00:00:00                        65                 96    98.783988  \n",
      "2023-01-01 01:00:00                        67                 95    99.186212  \n",
      "2023-01-01 02:00:00                        68                 94    98.800638  \n",
      "2023-01-01 03:00:00                        75                 96    98.349004  \n",
      "2023-01-01 04:00:00                        68                 95    98.643958  \n",
      "\n",
      "Patient vitals sample:\n",
      "           patient_id  temperature  heart_rate  blood_pressure_systolic  \\\n",
      "date                                                                      \n",
      "2023-01-01      P0001    98.389672          71                      119   \n",
      "2023-01-02      P0001    98.492046          67                      117   \n",
      "2023-01-03      P0001    98.790163          70                      113   \n",
      "2023-01-04      P0001    98.635781          74                      117   \n",
      "2023-01-05      P0001    98.051660          67                      118   \n",
      "\n",
      "            blood_pressure_diastolic     weight  \n",
      "date                                             \n",
      "2023-01-01                        84  68.996865  \n",
      "2023-01-02                        82  67.720215  \n",
      "2023-01-03                        78  67.846825  \n",
      "2023-01-04                        82  67.693993  \n",
      "2023-01-05                        83  68.228852  \n",
      "\n",
      "ICU patients: 20\n",
      "ICU date range: 2023-01-01 00:00:00 to 2023-06-29 23:00:00\n",
      "\n",
      "Patient vitals patients: 50\n",
      "Patient vitals date range: 2023-01-01 00:00:00 to 2023-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Load ICU monitoring data (hourly)\n",
    "icu_monitoring = pd.read_csv('data/icu_monitoring.csv')\n",
    "\n",
    "# Load patient vitals data (daily) - for comparison\n",
    "patient_vitals = pd.read_csv('data/patient_vitals.csv')\n",
    "\n",
    "print(\"ICU monitoring shape:\", icu_monitoring.shape)\n",
    "print(\"Patient vitals shape:\", patient_vitals.shape)\n",
    "\n",
    "# Convert datetime columns and set as index\n",
    "icu_monitoring['datetime'] = pd.to_datetime(icu_monitoring['datetime'])\n",
    "icu_monitoring = icu_monitoring.set_index('datetime')\n",
    "\n",
    "patient_vitals['date'] = pd.to_datetime(patient_vitals['date'])\n",
    "patient_vitals = patient_vitals.set_index('date')\n",
    "\n",
    "print(\"\\nICU monitoring sample:\")\n",
    "print(icu_monitoring.head())\n",
    "print(\"\\nPatient vitals sample:\")\n",
    "print(patient_vitals.head())\n",
    "\n",
    "# Check data characteristics\n",
    "print(f\"\\nICU patients: {icu_monitoring['patient_id'].nunique()}\")\n",
    "print(f\"ICU date range: {icu_monitoring.index.min()} to {icu_monitoring.index.max()}\")\n",
    "print(f\"\\nPatient vitals patients: {patient_vitals['patient_id'].nunique()}\")\n",
    "print(f\"Patient vitals date range: {patient_vitals.index.min()} to {patient_vitals.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7dd39",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Part 2.2: Time Series Selection\n",
    "\n",
    "**‚ö†Ô∏è WARNING: Sort Index Before Date Selection!**\n",
    "Since multiple patients share the same date, the `patient_vitals` index is non-monotonic (not strictly increasing). **You MUST sort the index first** before using `.loc` with date ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f905616",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "patient_vitals = patient_vitals.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551db0cf",
   "metadata": {},
   "source": [
    "Without sorting, pandas cannot reliably handle date range selections and may return unexpected results or errors.\n",
    "\n",
    "**TODO: Perform time series indexing and selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f8e2fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "January 1, 2023 data:            patient_id  temperature  heart_rate  blood_pressure_systolic  \\\n",
      "date                                                                      \n",
      "2023-01-01      P0001    98.389672          71                      119   \n",
      "2023-01-01      P0024    97.552103          71                      111   \n",
      "2023-01-01      P0025    98.806201          83                      118   \n",
      "2023-01-01      P0047    98.943464          63                      110   \n",
      "2023-01-01      P0026    98.758551          75                      114   \n",
      "2023-01-01      P0027    98.462467          90                      119   \n",
      "2023-01-01      P0028    99.632166          71                      117   \n",
      "2023-01-01      P0029    98.881121          68                      121   \n",
      "2023-01-01      P0030    98.403614          73                      122   \n",
      "2023-01-01      P0031    98.410626          63                      113   \n",
      "2023-01-01      P0046    98.507553          89                      123   \n",
      "2023-01-01      P0032    99.547822          70                      123   \n",
      "2023-01-01      P0033    97.612817          56                      112   \n",
      "2023-01-01      P0034    99.075484          88                      124   \n",
      "2023-01-01      P0035    98.968285          62                      116   \n",
      "2023-01-01      P0036    99.589629          85                      119   \n",
      "2023-01-01      P0037    98.557134          64                      117   \n",
      "2023-01-01      P0038    98.137036          62                      129   \n",
      "2023-01-01      P0045    98.265412          67                      119   \n",
      "2023-01-01      P0039    98.746267          88                      128   \n",
      "2023-01-01      P0023    99.142508          65                      118   \n",
      "2023-01-01      P0022    97.700397          68                      122   \n",
      "2023-01-01      P0021    99.549514          81                      125   \n",
      "2023-01-01      P0020    98.572122          64                      113   \n",
      "2023-01-01      P0004    99.214044          73                      126   \n",
      "2023-01-01      P0005    98.075744          61                      119   \n",
      "2023-01-01      P0006    98.373337          69                      113   \n",
      "2023-01-01      P0007    99.005033          81                      131   \n",
      "2023-01-01      P0050    99.008145          74                      130   \n",
      "2023-01-01      P0008    97.987253          82                      111   \n",
      "2023-01-01      P0009    99.368380          90                      128   \n",
      "2023-01-01      P0010    99.244181          86                      123   \n",
      "2023-01-01      P0011    98.829135          81                      124   \n",
      "2023-01-01      P0040    99.024034          77                      123   \n",
      "2023-01-01      P0012    97.820562          64                      121   \n",
      "2023-01-01      P0049    98.124573          66                      120   \n",
      "2023-01-01      P0014    98.405814          76                      121   \n",
      "2023-01-01      P0015    99.039286          81                      112   \n",
      "2023-01-01      P0016    99.201435          66                      115   \n",
      "2023-01-01      P0002    98.432983          87                      114   \n",
      "2023-01-01      P0017    99.146657          87                      115   \n",
      "2023-01-01      P0018    97.867184          87                      125   \n",
      "2023-01-01      P0019    99.026862          81                      123   \n",
      "2023-01-01      P0048    98.519258          75                      118   \n",
      "2023-01-01      P0013    98.662933          69                      110   \n",
      "2023-01-01      P0041    98.452086          84                      131   \n",
      "2023-01-01      P0003    98.167264          83                      110   \n",
      "2023-01-01      P0043    98.958357          79                      113   \n",
      "2023-01-01      P0042    98.881607          78                      120   \n",
      "2023-01-01      P0044    98.345119          66                      114   \n",
      "\n",
      "            blood_pressure_diastolic      weight  \n",
      "date                                              \n",
      "2023-01-01                        84   68.996865  \n",
      "2023-01-01                        70   49.386068  \n",
      "2023-01-01                        65   84.096164  \n",
      "2023-01-01                        76   54.318578  \n",
      "2023-01-01                        75   49.830300  \n",
      "2023-01-01                        74   72.943755  \n",
      "2023-01-01                        86  104.977255  \n",
      "2023-01-01                        71   59.883550  \n",
      "2023-01-01                        79   60.754351  \n",
      "2023-01-01                        75   72.486551  \n",
      "2023-01-01                        66   62.109514  \n",
      "2023-01-01                        72   74.585881  \n",
      "2023-01-01                        73   51.137556  \n",
      "2023-01-01                        76   66.779965  \n",
      "2023-01-01                        79   76.864597  \n",
      "2023-01-01                        76   81.704275  \n",
      "2023-01-01                        84   78.116801  \n",
      "2023-01-01                        85   40.000000  \n",
      "2023-01-01                        73   76.812239  \n",
      "2023-01-01                        77   59.093381  \n",
      "2023-01-01                        83   49.643152  \n",
      "2023-01-01                        79   68.067771  \n",
      "2023-01-01                        73   77.967356  \n",
      "2023-01-01                        77   70.714935  \n",
      "2023-01-01                        82   93.836456  \n",
      "2023-01-01                        86   86.238145  \n",
      "2023-01-01                        81   52.923008  \n",
      "2023-01-01                        75   56.926071  \n",
      "2023-01-01                        81   75.383257  \n",
      "2023-01-01                        68   68.629172  \n",
      "2023-01-01                        70   62.387123  \n",
      "2023-01-01                        77   59.678637  \n",
      "2023-01-01                        80   60.073705  \n",
      "2023-01-01                        77   79.612071  \n",
      "2023-01-01                        71   48.410507  \n",
      "2023-01-01                        73   48.085497  \n",
      "2023-01-01                        75   82.570817  \n",
      "2023-01-01                        72   65.294840  \n",
      "2023-01-01                        82   82.512213  \n",
      "2023-01-01                        74   58.179729  \n",
      "2023-01-01                        84   81.641348  \n",
      "2023-01-01                        86   67.832031  \n",
      "2023-01-01                        73   70.635756  \n",
      "2023-01-01                        80   63.274809  \n",
      "2023-01-01                        81   57.631518  \n",
      "2023-01-01                        84   49.964682  \n",
      "2023-01-01                        70   79.302945  \n",
      "2023-01-01                        75   65.165670  \n",
      "2023-01-01                        72   58.710525  \n",
      "2023-01-01                        69   72.543359  \n",
      "Records on Jan 1: 50 (some patients may start later)\n",
      "January 2023 shape: (1550, 6)\n",
      "\n",
      "First quarter average temperature: 98.97¬∞F\n",
      "After June average temperature: 98.41¬∞F\n",
      "First week average temperature: 98.68¬∞F\n",
      "Last week average temperature: 98.65¬∞F\n",
      "Business hours data shape: (32400, 6)\n",
      "\n",
      "Average heart rate - All hours: 81.7 bpm\n",
      "Average heart rate - Business hours: 80.7 bpm\n",
      "Average temperature - All hours: 98.5¬∞F\n",
      "Average temperature - Business hours: 98.5¬∞F\n"
     ]
    }
   ],
   "source": [
    "# TODO: Select data by specific dates\n",
    "# Note: Not all patients may have data on January 1, 2023 (some start later)\n",
    "# Important: Sort the index first since multiple patients share the same date\n",
    "patient_vitals = patient_vitals.sort_index()  # Sort for reliable date-based selection\n",
    "january_first = patient_vitals.loc['2023-01-01']\n",
    "  # Select January 1, 2023 from patient_vitals\n",
    "print(\"January 1, 2023 data:\", january_first)\n",
    "print(f\"Records on Jan 1: {len(january_first)} (some patients may start later)\")\n",
    "\n",
    "# TODO: Select data by date ranges\n",
    "january_data = patient_vitals.loc['2023-01-01':'2023-01-31']\n",
    " # Select entire January 2023\n",
    "print(\"January 2023 shape:\", january_data.shape)\n",
    "\n",
    "# TODO: Select data by time periods\n",
    "first_quarter = patient_vitals.loc['2023-01-01':'2023-03-31']\n",
    "# Select Q1 2023\n",
    "entire_year = patient_vitals.loc['2023']\n",
    "# Select all of 2023 (will include patients with partial year data)\n",
    "\n",
    "# TODO: Select first and last periods using .loc\n",
    "first_week = patient_vitals.loc[:patient_vitals.index.min() + pd.Timedelta(days=6)]  # First 7 days\n",
    "last_week = patient_vitals.loc[patient_vitals.index.max() - pd.Timedelta(days=6):]  # Last 7 days\n",
    "\n",
    "# TODO: Use truncate() method\n",
    "# Note: truncate() requires a sorted index. Sort first if needed: patient_vitals = patient_vitals.sort_index()\n",
    "data_after_june = patient_vitals.truncate(before='2023-06-01') \n",
    "# Truncate before June 1, 2023\n",
    "data_before_september = patient_vitals.truncate(after='2023-08-31')\n",
    " # Truncate after August 31, 2023\n",
    "\n",
    "# TODO: Use selected data for analysis\n",
    "# Compare average temperature between first quarter and data after June\n",
    "print(f\"\\nFirst quarter average temperature: {first_quarter['temperature'].mean():.2f}¬∞F\")\n",
    "print(f\"After June average temperature: {data_after_june['temperature'].mean():.2f}¬∞F\")\n",
    "print(f\"First week average temperature: {first_week['temperature'].mean():.2f}¬∞F\")\n",
    "print(f\"Last week average temperature: {last_week['temperature'].mean():.2f}¬∞F\")\n",
    "\n",
    "# For ICU data with time components:\n",
    "# TODO: Select business hours (9 AM to 5 PM)\n",
    "business_hours = icu_monitoring.between_time('09:00', '17:00')  \n",
    "# Use between_time()\n",
    "print(\"Business hours data shape:\", business_hours.shape)\n",
    "\n",
    "# TODO: Select specific time (noon readings)\n",
    "noon_data = icu_monitoring.at_time('12:00')\n",
    "# Use at_time('12:00')\n",
    "\n",
    "# TODO: Use time-based selection for analysis\n",
    "# Compare vital signs during business hours vs other times\n",
    "all_hours_avg = icu_monitoring.select_dtypes(include=[np.number]).mean()\n",
    "business_hours_avg = business_hours.select_dtypes(include=[np.number]).mean()\n",
    "print(f\"\\nAverage heart rate - All hours: {all_hours_avg['heart_rate']:.1f} bpm\")\n",
    "print(f\"Average heart rate - Business hours: {business_hours_avg['heart_rate']:.1f} bpm\")\n",
    "print(f\"Average temperature - All hours: {all_hours_avg['temperature']:.1f}¬∞F\")\n",
    "print(f\"Average temperature - Business hours: {business_hours_avg['temperature']:.1f}¬∞F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4653f2fe",
   "metadata": {},
   "source": [
    "## Part 2.3: Resampling Operations\n",
    "\n",
    "**TODO: Perform resampling and frequency conversion**\n",
    "\n",
    "**Important Note:** When resampling DataFrames that contain non-numeric columns (like `patient_id`), you'll get an error if you try to aggregate them with numeric functions like `mean()`. Use `df.select_dtypes(include=[np.number])` to select only numeric columns before resampling, or specify which columns to aggregate in `.agg()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d18466a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICU daily shape: (180, 5)\n",
      "Weekly resampled shape: (53, 5)\n",
      "Monthly resampled shape: (12, 5)\n",
      "ICU daily stats sample:\n",
      "           heart_rate                   temperature\n",
      "                 mean         max   min        mean\n",
      "datetime                                           \n",
      "2023-01-01  81.793729  111.829629  50.0   98.528348\n",
      "2023-01-02  81.479854  108.829629  50.0   98.498305\n",
      "2023-01-03  81.767332  110.330127  50.0   98.534337\n",
      "2023-01-04  81.852771  110.000000  50.0   98.542113\n",
      "2023-01-05  81.730187  109.829629  50.0   98.536312\n",
      "Missing values after upsampling: temperature                 323\n",
      "heart_rate                  323\n",
      "blood_pressure_systolic     323\n",
      "blood_pressure_diastolic    323\n",
      "weight                      323\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TODO: Resample hourly ICU data to daily\n",
    "# Note: Exclude non-numeric columns like 'patient_id' when resampling\n",
    "# Select only numeric columns before resampling\n",
    "numeric_cols = icu_monitoring.select_dtypes(include=[np.number]).columns\n",
    "icu_daily = icu_monitoring[numeric_cols].resample('D').mean()\n",
    "print(\"ICU daily shape:\", icu_daily.shape)\n",
    "\n",
    "# TODO: Resample daily patient data to weekly\n",
    "# Note: Exclude 'patient_id' column when resampling\n",
    "# Select only numeric columns before resampling\n",
    "numeric_cols_pv = patient_vitals.select_dtypes(include=[np.number]).columns\n",
    "patient_vitals_weekly = patient_vitals[numeric_cols_pv].resample('W').mean()\n",
    "print(\"Weekly resampled shape:\", patient_vitals_weekly.shape)\n",
    "\n",
    "# TODO: Resample daily patient data to monthly\n",
    "patient_vitals_monthly = patient_vitals[numeric_cols_pv].resample('ME').mean()\n",
    "# Resample to monthly with mean aggregation (use freq='ME' for Month End)\n",
    "print(\"Monthly resampled shape:\", patient_vitals_monthly.shape)\n",
    "\n",
    "# TODO: Use different aggregation functions (mean, sum, max, min)\n",
    "# Resample with multiple aggregations\n",
    "# Example: resample('D').agg({'heart_rate': ['mean', 'max', 'min'], \n",
    "#                             'temperature': 'mean'})\n",
    "icu_daily_stats = icu_monitoring[numeric_cols].resample('D').agg({'heart_rate': ['mean', 'max', 'min'],\n",
    "                                                     'temperature': 'mean'})\n",
    "print(\"ICU daily stats sample:\")\n",
    "print(icu_daily_stats.head())\n",
    "\n",
    "# TODO: Handle missing values during resampling\n",
    "# Demonstrate upsampling (monthly to daily) creates missing values\n",
    "# Note: When upsampling, use .asfreq() to create missing values, or use .resample() with aggregation\n",
    "monthly_to_daily = patient_vitals_monthly.resample('D').asfreq() \n",
    "# Upsample monthly data to daily (use .asfreq() or .resample('D'))\n",
    "print(\"Missing values after upsampling:\", monthly_to_daily.isna().sum())\n",
    "\n",
    "# TODO: Compare different resampling frequencies\n",
    "# Create a DataFrame comparing resampling results at different frequencies\n",
    "# Important: Since patient_vitals contains multiple patients per date, you need to aggregate by date first\n",
    "# to create a single daily time series for comparison.\n",
    "# Why aggregation is needed: The patient_vitals DataFrame has multiple rows per date (one for each patient),\n",
    "# so we need to average across patients for each date to create a single daily time series that can be\n",
    "# meaningfully compared with the weekly and monthly resampled data. Without aggregation, resampling would\n",
    "# operate on each patient's time series separately, making it difficult to compare frequencies meaningfully.\n",
    "# Steps:\n",
    "# 1. Since 'date' is currently the index, reset it to a column first, then aggregate by date\n",
    "#    Note: groupby('date').mean() automatically sets 'date' as the index in the result, so you don't need\n",
    "#    to call set_index('date') again after groupby.\n",
    "patient_vitals_reset = patient_vitals[numeric_cols_pv].reset_index()\n",
    "patient_vitals_daily_agg = patient_vitals_reset.groupby('date').mean()\n",
    "#    # The date is already the index after groupby, so no need to set_index again\n",
    "# 2. Compare the aggregated daily data with weekly and monthly resampled data\n",
    "# Use patient_vitals data resampled to different frequencies:\n",
    "# - Original daily data (aggregated by date): patient_vitals_daily_agg\n",
    "# - Weekly resampled (patient_vitals_weekly) \n",
    "# - Monthly resampled (patient_vitals_monthly)\n",
    "# Include columns: frequency, date_range, row_count, mean_temperature, std_temperature\n",
    "# Use the 'temperature' column from each resampled dataset\n",
    "# Example structure:\n",
    "resampling_comparison = pd.DataFrame({\n",
    "    \"frequency\": [\"daily\", \"weekly\", \"monthly\"],\n",
    "    \"date_range\": [\n",
    "        f\"{patient_vitals_daily_agg.index.min().date()} ‚Äì {patient_vitals_daily_agg.index.max().date()}\",\n",
    "        f\"{patient_vitals_weekly.index.min().date()} ‚Äì {patient_vitals_weekly.index.max().date()}\",\n",
    "        f\"{patient_vitals_monthly.index.min().date()} ‚Äì {patient_vitals_monthly.index.max().date()}\",\n",
    "    ],\n",
    "    \"row_count\": [\n",
    "        len(patient_vitals_daily_agg),\n",
    "        len(patient_vitals_weekly),\n",
    "        len(patient_vitals_monthly),\n",
    "    ],\n",
    "    \"mean_temperature\": [\n",
    "        patient_vitals_daily_agg[\"temperature\"].mean(),\n",
    "        patient_vitals_weekly[\"temperature\"].mean(),\n",
    "        patient_vitals_monthly[\"temperature\"].mean(),\n",
    "    ],\n",
    "    \"std_temperature\": [\n",
    "        patient_vitals_daily_agg[\"temperature\"].std(),\n",
    "        patient_vitals_weekly[\"temperature\"].std(),\n",
    "        patient_vitals_monthly[\"temperature\"].std(),\n",
    "    ],\n",
    "})  \n",
    "\n",
    "# TODO: Save results as 'output/q2_resampling_analysis.csv'\n",
    "resampling_comparison.to_csv('output/q2_resampling_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3605486d",
   "metadata": {},
   "source": [
    "## Part 2.4: Missing Data Handling\n",
    "\n",
    "**üí° TIP: High Percentage of Missing Data is Expected!**\n",
    "When upsampling from monthly to daily frequency, you'll create approximately 96% missing data (only 12 month-end dates have values out of 365 days). This is normal and expected for upsampling - don't be alarmed!\n",
    "\n",
    "**Approach:** Create missing values by upsampling monthly data to daily frequency. This creates a clear, structured pattern of missing data that's ideal for practicing imputation methods.\n",
    "\n",
    "**TODO: Handle missing data in time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9dd87022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value count: 323\n",
      "Missing value percentage: 96.41791044776119\n"
     ]
    }
   ],
   "source": [
    "# TODO: Identify missing values in time series\n",
    "# Use the monthly resampled data from Part 2.3 and upsample to daily:\n",
    "#   - Take patient_vitals_monthly['temperature']\n",
    "#   - Upsample to daily frequency using .resample('D').asfreq()\n",
    "#   - This creates missing values for all days except month-end dates (~96% missing)\n",
    "ts_with_missing = patient_vitals_monthly['temperature'].resample('D').asfreq()\n",
    "# Time series with missing values\n",
    "print(\"Missing value count:\", ts_with_missing.isna().sum())\n",
    "print(\"Missing value percentage:\", ts_with_missing.isna().sum() / len(ts_with_missing) * 100)\n",
    "\n",
    "# TODO: Use forward fill and backward fill\n",
    "ts_ffill = ts_with_missing.ffill() \n",
    "# Forward fill missing values (use .ffill() method)\n",
    "ts_bfill = ts_with_missing.bfill() \n",
    "# Backward fill missing values (use .bfill() method)\n",
    "\n",
    "# TODO: Use interpolation methods\n",
    "ts_interpolated = ts_with_missing.interpolate() \n",
    "# Interpolate missing values\n",
    "ts_interpolated_linear = ts_with_missing.interpolate(method='linear') \n",
    "# Linear interpolation\n",
    "ts_interpolated_time = ts_with_missing.interpolate(method='time') \n",
    "# Time-based interpolation\n",
    "\n",
    "# TODO: Use rolling mean for imputation\n",
    "ts_rolling_imputed = ts_with_missing.rolling(window=3, min_periods=1).mean()  \n",
    "# Fill missing with rolling mean\n",
    "\n",
    "# TODO: Create missing data report\n",
    "# Document your missing data handling with the following sections:\n",
    "# 1. Missing value summary: Total count and percentage\n",
    "# 2. Missing data patterns: When/why data is missing (by month, day of week, etc.)\n",
    "# 3. Imputation method: Which method you used (forward fill, backward fill, interpolation, rolling mean)\n",
    "# 4. Rationale: Why you chose that method\n",
    "# 5. Pros and cons: Advantages and limitations of your approach\n",
    "# 6. Example: Show at least one example of missing data before and after imputation\n",
    "# Minimum length: 300 words\n",
    "missing_data_report = f\"\"\"\n",
    "Missing Data Report\n",
    "\n",
    "Missing Value Summary:\n",
    "Missing Values Total Count: {ts_with_missing.isna().sum()}\n",
    "Percentage Missing: {ts_with_missing.isna().sum() / len(ts_with_missing) * 100:.2f}%%\n",
    "The percentage of missing data is quite high because of the upsampling process.\n",
    "\n",
    "Imputation Method: Forward fill (ffill) and Backward fill (bfill)\n",
    "Rationale:\n",
    "I chose forward fill and backward fill methods because they are straightforward and effective for time series data where values tend to remain stable over short periods. By carrying forward the last known value and filling backward from the next known value, we can maintain continuity in the data without introducing abrupt changes that might occur with other methods like interpolation.\n",
    "Pros and Cons:\n",
    "Pros:\n",
    "1. Easy to implement and understand.\n",
    "2. Preserves existing data points because it does not introduce new values that could change trends.\n",
    "Cons:\n",
    "1. Not good for data with quick changes as it may lead to extra periods of repeated values in datasets with high volatility.\n",
    "Example of Missing Data Before and After Imputation:   \n",
    "Before Imputation:\n",
    "{ts_with_missing.head(10)}\n",
    "After Forward Fill Imputation:\n",
    "{ts_ffill.head(10)}\n",
    "After Backward Fill Imputation:\n",
    "{ts_bfill.head(10)}\n",
    "\n",
    "When/Why Data is Missing:\n",
    "Only January had no missing data, and the months after all had missing data. It could be data is only collected at the end of each month.\n",
    "For the day values, almost every day of the week had missing data. But the missing data seems to be evenly distributed across the week days.\n",
    "The missing values are caused by upsampling monthly data to daily frequency. The original dataset has daily measurements only for January, but the other months likely only has one observation per month. \n",
    "When resampled to daily, we generated a full daily time index for each month, resulting in 29‚Äì30 missing values for every month after January.\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Document missing data patterns\n",
    "# Analyze when/why data is missing\n",
    "missing_by_month = ts_with_missing.groupby(ts_with_missing.index.month).apply(lambda x: x.isna().sum())\n",
    "missing_by_day = ts_with_missing.groupby(ts_with_missing.index.dayofweek).apply(lambda x: x.isna().sum())\n",
    "missing_patterns = f\"Missing by month:\\n{missing_by_month}\\n\\nMissing by day of week:\\n{missing_by_day}\"\n",
    "\n",
    "# TODO: Save results as 'output/q2_missing_data_report.txt'\n",
    "with open('output/q2_missing_data_report.txt', 'w') as f:\n",
    "     f.write(missing_data_report)\n",
    "     f.write(f\"\\n\\nMissing patterns:\\n{missing_patterns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf350d",
   "metadata": {},
   "source": [
    "## Submission Checklist\n",
    "\n",
    "Before moving to Question 3, verify you've created:\n",
    "\n",
    "- [X] `output/q2_resampling_analysis.csv` - resampling analysis results\n",
    "- [X] `output/q2_missing_data_report.txt` - missing data handling report\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
